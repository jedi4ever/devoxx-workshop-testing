{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG using langchain\n",
    "## Introduction\n",
    "- In this lesson we want to use the information in our vector database to answer questions.\n",
    "- This concept of retrieving information based on a question is called RAG or Retrieval Augmented Generation. (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --upgrade langchain langchain_community langchain_openai chromadb langchain-chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup embeddings model\n",
    "- We setup the embeddings model, we use the same model that used for indexing the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "print(embeddings.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the vector database\n",
    "- We now connect to the vector database that has persistent storage.\n",
    "- And set the right collection name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "#from langchain_community.vectorstores import Chroma\n",
    "\n",
    "collection_name=\"documentation\"\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chromadb\")\n",
    "\n",
    "# Vectory database will calculate them using the embeddings_model provided\n",
    "# and store the embeddings for each doc in it's database\n",
    "\n",
    "vectordb = Chroma(\n",
    "    client = chroma_client, collection_name = collection_name, embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the vector database to a retriever\n",
    "- To use the vector database as part of a chain, we need to convert it to a retriever.\n",
    "- We can set the number of results we want to retrieve and the threshold for the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='5f95f5b1-6e65-4451-bd53-5e3b3ad67814', metadata={}, page_content='We use REST API to communicate with the server.'), Document(id='76388bf2-20a3-4f53-a7f4-cec237363f99', metadata={}, page_content='We prefer snake_case for naming variables over CamelCase in javascript.'), Document(id='3eb6ce36-b568-45f1-978e-a2ea98c3da0a', metadata={}, page_content='For testing in javascript we use Jest.')]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# retriever = db.as_retriever)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4}, threshold=0.7)\n",
    "\n",
    "# https://python.langchain.com/docs/how_to/sequence/\n",
    "docs = retriever.invoke(\"What are the architectural guidelines for this app ?\")\n",
    "print(docs)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The RAG prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we retrieve all the documents relate to the query, we can add them to the prompt.\n",
    "- Note that we add `answer the question based only on the following context:` to the prompt.\n",
    "- This instructs the llm to only use the context to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n\\n    {context}\\n\\n    Question: {question}\\n    '), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the chain\n",
    "The whole chain now looks like this:\n",
    "- The retriever finds all documents related to our query from the vectordatabase.\n",
    "- This is set as the `context` in the prompt.\n",
    "- Then the whole prompt gets send over to the llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "chain = {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG vs LLM\n",
    "- We can use the llm directly to answer the question.\n",
    "- But the RAG will use the context to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM:  Certainly! Here's a simple \"Hello, World!\" function in JavaScript:\n",
      "\n",
      "```javascript\n",
      "function sayHello() {\n",
      "  console.log(\"Hello, World!\");\n",
      "}\n",
      "\n",
      "// Call the function to display the message\n",
      "sayHello();\n",
      "```\n",
      "\n",
      "This function, `sayHello`, when called, will print \"Hello, World!\" to the console. You can run this code in any JavaScript environment, such as a web browser's developer console or Node.js.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM + RAG ```javascript\n",
      "function hello_world() {\n",
      "    console.log(\"Hello, World!\");\n",
      "}\n",
      "\n",
      "hello_world();\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.debug = False\n",
    "\n",
    "question = \"Write a hello world function in Javascript\"\n",
    "result = llm.invoke(question)\n",
    "print(\"LLM: \", result.content)\n",
    "result = chain.invoke(question)\n",
    "print(\"LLM + RAG\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under the hood\n",
    "- We use a global langchain debug to see what goes on under the hood.\n",
    "- This allows us to the populated prompt and the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Write a hello world function in Javascript\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Write a hello world function in Javascript\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Write a hello world function in Javascript\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Write a hello world function in Javascript\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Write a hello world function in Javascript\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:format_docs] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:format_docs] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"For testing in javascript we use Jest.\\n\\nWe prefer snake_case for naming variables over CamelCase in javascript.\\n\\nWe use REST API to communicate with the server.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence] [313ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"For testing in javascript we use Jest.\\n\\nWe prefer snake_case for naming variables over CamelCase in javascript.\\n\\nWe use REST API to communicate with the server.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] [314ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": \"For testing in javascript we use Jest.\\n\\nWe prefer snake_case for naming variables over CamelCase in javascript.\\n\\nWe use REST API to communicate with the server.\",\n",
      "  \"question\": \"Write a hello world function in Javascript\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"context\": \"For testing in javascript we use Jest.\\n\\nWe prefer snake_case for naming variables over CamelCase in javascript.\\n\\nWe use REST API to communicate with the server.\",\n",
      "  \"question\": \"Write a hello world function in Javascript\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the question based only on the following context:\\n\\n    For testing in javascript we use Jest.\\n\\nWe prefer snake_case for naming variables over CamelCase in javascript.\\n\\nWe use REST API to communicate with the server.\\n\\n    Question: Write a hello world function in Javascript\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [2.27s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Certainly! Here's a simple \\\"Hello, World!\\\" function in JavaScript using snake_case for naming:\\n\\n```javascript\\nfunction say_hello_world() {\\n    console.log('Hello, World!');\\n}\\n```\\n\\nYou can call this function by using `say_hello_world();` and it will print \\\"Hello, World!\\\" to the console.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Certainly! Here's a simple \\\"Hello, World!\\\" function in JavaScript using snake_case for naming:\\n\\n```javascript\\nfunction say_hello_world() {\\n    console.log('Hello, World!');\\n}\\n```\\n\\nYou can call this function by using `say_hello_world();` and it will print \\\"Hello, World!\\\" to the console.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 69,\n",
      "                \"prompt_tokens\": 61,\n",
      "                \"total_tokens\": 130,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "              \"system_fingerprint\": \"fp_90122d973c\",\n",
      "              \"id\": \"chatcmpl-BUZ7xbtU5rPH2pJsaKnVM1HmRNSSK\",\n",
      "              \"service_tier\": \"default\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--216948a5-aa18-4eca-92df-80380891a11c-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 61,\n",
      "              \"output_tokens\": 69,\n",
      "              \"total_tokens\": 130,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 69,\n",
      "      \"prompt_tokens\": 61,\n",
      "      \"total_tokens\": 130,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "    \"system_fingerprint\": \"fp_90122d973c\",\n",
      "    \"id\": \"chatcmpl-BUZ7xbtU5rPH2pJsaKnVM1HmRNSSK\",\n",
      "    \"service_tier\": \"default\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Certainly! Here's a simple \\\"Hello, World!\\\" function in JavaScript using snake_case for naming:\\n\\n```javascript\\nfunction say_hello_world() {\\n    console.log('Hello, World!');\\n}\\n```\\n\\nYou can call this function by using `say_hello_world();` and it will print \\\"Hello, World!\\\" to the console.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [2.58s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Certainly! Here's a simple \\\"Hello, World!\\\" function in JavaScript using snake_case for naming:\\n\\n```javascript\\nfunction say_hello_world() {\\n    console.log('Hello, World!');\\n}\\n```\\n\\nYou can call this function by using `say_hello_world();` and it will print \\\"Hello, World!\\\" to the console.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.debug = True\n",
    "result = chain.invoke(question)\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grounding\n",
    "- We can either override knowledge from the llm using RAG.\n",
    "- Or also add our own knowledge and avoid having to train/fine tune the model\n",
    "- Consider the following example that we only want to answer if it is part of our knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of using CamelCase for function names depends on the coding standards and style guidelines adopted by your team or organization. In some programming languages and communities, CamelCase is preferred for naming functions, while in others, a different convention like snake_case is used.\n",
      "\n",
      "Here are a few common conventions:\n",
      "\n",
      "- **CamelCase**: Commonly used in languages like Java and C#. Example: `CalculateInterestRate()`\n",
      "  \n",
      "- **PascalCase**: Similar to CamelCase but often used interchangeably in contexts where the first letter is also capitalized. Example: `FetchUserData()`\n",
      "\n",
      "- **snake_case**: Preferred in languages like Python. Example: `calculate_interest_rate()`\n",
      "\n",
      "When deciding on a naming convention, consider the following:\n",
      "\n",
      "1. **Language Guidelines**: Check if the language has its own recommended convention.\n",
      "2. **Team Standards**: Align with your teamâ€™s existing code style for consistency.\n",
      "3. **Readability**: Choose a style that makes your code easy to read and understand.\n",
      "\n",
      "If you're starting a new project, it's a good idea to establish a style guide early on to ensure consistency across your codebase.\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"Are we using CamelCase for our function names ?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context provided does not specifically mention a preference for function naming conventions in JavaScript, only that snake_case is preferred for variable names. Therefore, it does not definitively specify whether CamelCase should be used for function names. Preferences for function naming may depend on specific style guides or individual/team preferences beyond what is mentioned in the context.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"Should we using CamelCase for function names in javascript?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context provided does not specify the authentication framework used in the app.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"What is the authentication framework used in this app?\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
