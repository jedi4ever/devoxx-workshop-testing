{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG using langchain\n",
    "## Introduction\n",
    "- In this lesson we want to use the information in our vector database to answer questions.\n",
    "- This concept of retrieving information based on a question is called RAG or Retrieval Augmented Generation. (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --upgrade langchain langchain_community langchain_openai chromadb langchain-chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup embeddings model\n",
    "- We setup the embeddings model, we use the same model that used for indexing the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "print(embeddings.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the vector database\n",
    "- We now connect to the vector database that has persistent storage.\n",
    "- And set the right collection name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "#from langchain_community.vectorstores import Chroma\n",
    "\n",
    "collection_name=\"documentation\"\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chromadb\")\n",
    "\n",
    "# Vectory database will calculate them using the embeddings_model provided\n",
    "# and store the embeddings for each doc in it's database\n",
    "\n",
    "vectordb = Chroma(\n",
    "    client = chroma_client, collection_name = collection_name, embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the vector database to a retriever\n",
    "- To use the vector database as part of a chain, we need to convert it to a retriever.\n",
    "- We can set the number of results we want to retrieve and the threshold for the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='af2aebf1-59ea-411e-8945-b2eda964641a', metadata={}, page_content='We use REST API to communicate with the server.'), Document(id='9dd6ef06-ee7c-4401-b5b3-77c71ffcd5a5', metadata={}, page_content='We prefer snake_case for naming variables over CamelCase in javascript.'), Document(id='c94fe47c-ee03-44ff-8d41-a0d1f599cc7f', metadata={}, page_content='For testing in javascript we use Jest.')]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# retriever = db.as_retriever)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4}, threshold=0.7)\n",
    "\n",
    "# https://python.langchain.com/docs/how_to/sequence/\n",
    "docs = retriever.invoke(\"What are the architectural guidelines for this app ?\")\n",
    "print(docs)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The RAG prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we retrieve all the documents relate to the query, we can add them to the prompt.\n",
    "- Note that we add `answer the question based only on the following context:` to the prompt.\n",
    "- This instructs the llm to only use the context to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n\\n    {context}\\n\\n    Question: {question}\\n    '), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the chain\n",
    "The whole chain now looks like this:\n",
    "- The retriever finds all documents related to our query from the vectordatabase.\n",
    "- This is set as the `context` in the prompt.\n",
    "- Then the whole prompt gets send over to the llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "chain = {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG vs LLM\n",
    "- We can use the llm directly to answer the question.\n",
    "- But the RAG will use the context to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM:  Certainly! Here's a simple \"Hello, World!\" function in JavaScript:\n",
      "\n",
      "```javascript\n",
      "function sayHello() {\n",
      "    console.log(\"Hello, World!\");\n",
      "}\n",
      "\n",
      "sayHello();\n",
      "```\n",
      "\n",
      "This function, `sayHello`, when called, will output \"Hello, World!\" to the console. You can run this code in any JavaScript environment, such as in the browser's developer console or a Node.js environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM + RAG Based on the provided context, here is a simple \"Hello, World!\" function in JavaScript, following the preference for `snake_case` for variable names:\n",
      "\n",
      "```javascript\n",
      "function say_hello_world() {\n",
      "    console.log('Hello, World!');\n",
      "}\n",
      "\n",
      "say_hello_world();\n",
      "```\n",
      "\n",
      "This function defines `say_hello_world` using `snake_case` and logs \"Hello, World!\" to the console.\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.debug = False\n",
    "\n",
    "question = \"Write a hello world function in Javascript\"\n",
    "result = llm.invoke(question)\n",
    "print(\"LLM: \", result.content)\n",
    "result = chain.invoke(question)\n",
    "print(\"LLM + RAG\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under the hood\n",
    "- We use a global langchain debug to see what goes on under the hood.\n",
    "- This allows us to the populated prompt and the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Write a hello world function in Javascript\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Write a hello world function in Javascript\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Write a hello world function in Javascript\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Write a hello world function in Javascript\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Write a hello world function in Javascript\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:format_docs] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:format_docs] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"For testing in javascript we use Jest.\\n\\nWe prefer snake_case for naming variables over CamelCase in javascript.\\n\\nWe use REST API to communicate with the server.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence] [446ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"For testing in javascript we use Jest.\\n\\nWe prefer snake_case for naming variables over CamelCase in javascript.\\n\\nWe use REST API to communicate with the server.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] [446ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": \"For testing in javascript we use Jest.\\n\\nWe prefer snake_case for naming variables over CamelCase in javascript.\\n\\nWe use REST API to communicate with the server.\",\n",
      "  \"question\": \"Write a hello world function in Javascript\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"context\": \"For testing in javascript we use Jest.\\n\\nWe prefer snake_case for naming variables over CamelCase in javascript.\\n\\nWe use REST API to communicate with the server.\",\n",
      "  \"question\": \"Write a hello world function in Javascript\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the question based only on the following context:\\n\\n    For testing in javascript we use Jest.\\n\\nWe prefer snake_case for naming variables over CamelCase in javascript.\\n\\nWe use REST API to communicate with the server.\\n\\n    Question: Write a hello world function in Javascript\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [908ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Based on the provided context, here's a simple \\\"Hello World\\\" function in JavaScript using snake_case for naming:\\n\\n```javascript\\nfunction hello_world() {\\n    console.log(\\\"Hello, World!\\\");\\n}\\n\\nhello_world();\\n```\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Based on the provided context, here's a simple \\\"Hello World\\\" function in JavaScript using snake_case for naming:\\n\\n```javascript\\nfunction hello_world() {\\n    console.log(\\\"Hello, World!\\\");\\n}\\n\\nhello_world();\\n```\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 45,\n",
      "                \"prompt_tokens\": 61,\n",
      "                \"total_tokens\": 106,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "              \"system_fingerprint\": \"fp_90122d973c\",\n",
      "              \"id\": \"chatcmpl-BPrOEbhOkcEcXMIreKlh61tTl1NIZ\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-44026c6c-f8bf-4c38-a7c7-5f294df490a2-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 61,\n",
      "              \"output_tokens\": 45,\n",
      "              \"total_tokens\": 106,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 45,\n",
      "      \"prompt_tokens\": 61,\n",
      "      \"total_tokens\": 106,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "    \"system_fingerprint\": \"fp_90122d973c\",\n",
      "    \"id\": \"chatcmpl-BPrOEbhOkcEcXMIreKlh61tTl1NIZ\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Based on the provided context, here's a simple \\\"Hello World\\\" function in JavaScript using snake_case for naming:\\n\\n```javascript\\nfunction hello_world() {\\n    console.log(\\\"Hello, World!\\\");\\n}\\n\\nhello_world();\\n```\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [1.36s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Based on the provided context, here's a simple \\\"Hello World\\\" function in JavaScript using snake_case for naming:\\n\\n```javascript\\nfunction hello_world() {\\n    console.log(\\\"Hello, World!\\\");\\n}\\n\\nhello_world();\\n```\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.debug = True\n",
    "result = chain.invoke(question)\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grounding\n",
    "- We can either override knowledge from the llm using RAG.\n",
    "- Or also add our own knowledge and avoid having to train/fine tune the model\n",
    "- Consider the following example that we only want to answer if it is part of our knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whether you should use CamelCase for function names depends on the coding standards and conventions of the particular programming language you are using, as well as the guidelines set by your development team or organization. \n",
      "\n",
      "Here are some general conventions:\n",
      "\n",
      "1. **Python**: The PEP 8 style guide recommends using `snake_case` for function names.\n",
      "   \n",
      "2. **JavaScript**: The common convention is to use `camelCase` for function names.\n",
      "   \n",
      "3. **Java**: The convention is also to use `camelCase` for method names.\n",
      "   \n",
      "4. **C#**: It is typical to use `PascalCase` for method names, which is similar to CamelCase but with the first letter capitalized too.\n",
      "\n",
      "5. **C++**: There is no single standard, but both `camelCase` and `PascalCase` are widely used, depending on the project or team standards.\n",
      "\n",
      "It's important to consistently follow the conventions adopted by your project or team to ensure code readability and maintainability. If there's no set guideline, choose a style and apply it consistently throughout your codebase.\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"Are we using CamelCase for our function names ?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context, the preference is for using snake_case for naming variables in JavaScript. There is no specific mention of function naming conventions in the context provided. However, given the preference for snake_case for variables, it might suggest a similar preference for consistency. Therefore, it would be reasonable to consider using snake_case for function names as well, unless a different convention is specified for functions elsewhere.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"Should we using CamelCase for function names in javascript?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context provided does not specify which authentication framework is used in the app.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"What is the authentication framework used in this app?\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
