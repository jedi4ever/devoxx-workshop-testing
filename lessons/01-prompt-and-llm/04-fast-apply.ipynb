{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c5692e",
   "metadata": {},
   "source": [
    "# Fast Apply\n",
    "## Introduction\n",
    "\n",
    "Frontier models such as GPT-4o struggle on large edits, with problems of laziness, inaccuracy, and high-latency.\n",
    "\n",
    "This is a weakness visible in coding agents. Accurately editing hundreds of lines can take multiple model calls, at times trapping the agent in an infinite loop. Even small, isolated edits are plagued with bugs.\n",
    "\n",
    "Worst of all, existing models are slow at large edits, breaking the programmer out of flow. We've trained a specialized model on an important version of the full-file code edit task called fast apply.\n",
    "\n",
    "The concept of Fast applies was first introduced by Cursor:\n",
    "- https://web.archive.org/web/20240823050616/https://www.cursor.com/blog/instant-apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbc2b4",
   "metadata": {},
   "source": [
    "There are few articles available about it , but none really has details:\n",
    "- https://fireworks.ai/blog/cursor\n",
    "- https://github.com/llllvvuu/instant_apply\n",
    "- https://github.com/paritoshk/anysphere_test?tab=readme-ov-file\n",
    "\n",
    "Luckily someone made an Open Source version , which we'll explore\n",
    "- https://github.com/kortix-ai/fast-apply\n",
    "- https://www.reddit.com/r/LocalLLaMA/comments/1ga25gj/introducing_fast_apply_replicate_cursors_instant/\n",
    "- https://github.com/kortix-ai/fast-apply/tree/main/tests_evaluate/example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce596ec4",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45ede6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU transformers torch colorama\n",
    "import sys\n",
    "sys.path.append('./lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2865815c",
   "metadata": {},
   "source": [
    "## Model on HuggingFace\n",
    "The model we're using is <https://huggingface.co/Kortix/FastApply-7B-v1.0>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052570a6",
   "metadata": {},
   "source": [
    "### Setting up the model\n",
    "! This model requires a lot of memory and is not ideal to be run in codespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d341ac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Kortix/FastApply-7B-v1.0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Kortix/FastApply-7B-v1.0\")\n",
    "\n",
    "import torch\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model= model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725912f",
   "metadata": {},
   "source": [
    "## Setup the existing and refactor suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01eb0949",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_code = \"\"\"\n",
    "// These functions help print the answers\n",
    "function my_function() {\n",
    "    helloWorld();\n",
    "}\n",
    "\n",
    "// This function prints \"Hello, World!\"\n",
    "function helloWorld() {\n",
    "    console.log(\"Hello, World!\");\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a1e83c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "refactor_suggestion = \"\"\"\n",
    "function myFunction() {\n",
    "    helloWorld(\"Hello, World!\");\n",
    "}\n",
    "\n",
    "function helloWorld(message) {\n",
    "    console.log(message);\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2383c176",
   "metadata": {},
   "source": [
    "## The instruct prompt\n",
    "- You can see `|im_start|` and `<|im_end|>` tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ff959639",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<|im_start|>system\n",
    "You are a coding assistant that helps merge code updates, ensuring every modification is fully integrated.<|im_end|>\n",
    "<|im_start|>user\n",
    "Merge all changes from the <update> snippet into the <code> below.\n",
    "- Preserve the code's structure, order, comments, and indentation exactly.\n",
    "- Output only the updated code, enclosed within <updated-code> and </updated-code> tags.\n",
    "- Do not include any additional text, explanations, placeholders, ellipses, or code fences.\n",
    "\n",
    "<code>{existing_code}</code>\n",
    "\n",
    "<update>{refactor_suggestion}</update>\n",
    "\n",
    "Provide the complete updated code.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    existing_code=existing_code,\n",
    "    refactor_suggestion=refactor_suggestion,\n",
    ").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff476a",
   "metadata": {},
   "source": [
    "## Generate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4d1fbfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response:\n",
      "\n",
      "<updated-code>// These functions help print the answers\n",
      "function myFunction() {\n",
      "    helloWorld(\"Hello, World!\");\n",
      "}\n",
      "\n",
      "// This function prints \"Hello, World!\"\n",
      "function helloWorld(message) {\n",
      "    console.log(message);\n",
      "}</updated-code><|im_end|>\n",
      "====================\n",
      "// These functions help print the answers\n",
      "function myFunction() {\n",
      "    helloWorld(\"Hello, World!\");\n",
      "}\n",
      "\n",
      "// This function prints \"Hello, World!\"\n",
      "function helloWorld(message) {\n",
      "    console.log(message);\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(input_ids, max_length=8192,)\n",
    "response = tokenizer.decode(output[0][len(input_ids[0]):])\n",
    "print(\"Full response:\")\n",
    "print(response)\n",
    "print(20*\"=\")\n",
    "updated_code = response.split(\"<updated-code>\")[1].split(\"</updated-code>\")[0]\n",
    "print(updated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ca387",
   "metadata": {},
   "source": [
    "## A diff helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "867b85dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[47m\u001b[30m\u001b[41m\n",
      "\u001b[47m// These functions help print the answers\n",
      "function my\u001b[42mF\u001b[47m\u001b[41m_f\u001b[47munction() {\n",
      "    helloWorld(\u001b[42m\"Hello, World!\"\u001b[47m);\n",
      "}\n",
      "\n",
      "// This function prints \"Hello, World!\"\n",
      "function helloWorld(\u001b[42mmessage\u001b[47m) {\n",
      "    console.log(\u001b[42mmessage\u001b[47m\u001b[41m\"Hello, World!\"\u001b[47m);\n",
      "}\u001b[41m\n",
      "\u001b[47m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from diff_helper import diff_code\n",
    "print(diff_code(existing_code, updated_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669535b",
   "metadata": {},
   "source": [
    "You can see there is more to building an AI Coding IDE then just asking gpt models !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
