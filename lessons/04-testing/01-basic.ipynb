{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect - an Eval Framework\n",
    "## Introduction\n",
    "- As we've seen in the lesson about simple testing, changing the prompt slightly and senting it to the same llm can give different results.\n",
    "- Imagine swapping in and out another model, how do you make sure your code keeps working?\n",
    "- Much like in Test Driven Development, the LLM community talks about *Evals*\n",
    "- Evals can be seen as a test suite to check the results across multiple prompts and llms\n",
    "- We've seen how it works, but luckily there exist frameworks that have some helpers , so we don't have to do it all by ourselves\n",
    "\n",
    "The framework we will show here is `Inspect AI` - <https://inspect.ai-safety-institute.org.uk/>. It comes in the form of a VSCode plugin too, this is eanbled in this workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : we are pinning the inspect_ai due to a recent breaking change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai anthropic ipywidgets colorama\n",
    "import os\n",
    "os.environ['XDG_RUNTIME_DIR'] = \"/tmp\"\n",
    "os.environ['INSPECT_EVAL_MODEL'] = \"openai/gpt-4o-mini\"\n",
    "\n",
    "from helpers.reporter.pretty import pretty_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your first solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec4b25a4b4b48a7b0330029d095cd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-07T10-52-38+00-00_simple-generate_F2348NyvYCavcXbfgUq4oE.eval\n",
      "======= Sample ========\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: \n",
      "====================\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Sure! Here’s the content of a simple JavaScript file named `hello-world.js` that prints out \"hello\":\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "To create this file:\n",
      "\n",
      "1. Open a text editor or an IDE of your choice.\n",
      "2. Copy the above code.\n",
      "3. Save the file with the name `hello-world.js`.\n",
      "\n",
      "You can run this script using Node.js or in a browser's console. If using Node.js, simply run the following command in your terminal:\n",
      "\n",
      "```bash\n",
      "node hello-world.js\n",
      "```\n",
      "\n",
      "This will output:\n",
      "\n",
      "```\n",
      "hello\n",
      "```\n",
      "======== Score ======\n",
      "====================\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import generate\n",
    "\n",
    "@task\n",
    "def simple_generate() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            generate()\n",
    "        ],\n",
    "    )\n",
    "\n",
    "results = eval(simple_generate, max_steps=10,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9c0f535a5a48ce9753439672d2a91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-07T10-52-41+00-00_system-message-solver_Sps8AtzSdJA8xvyXeuqhYv.eval\n",
      "======= Sample ========\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: \n",
      "====================\n",
      "\u001b[33m system     \u001b[39m> You are a code generation assistant. Your task is to generate code based on the input provided.\n",
      "    You should ensure that the generated code meets the requirements specified in the input.\n",
      "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
      "    Make sure to output the code and not any additional text or comments.\n",
      "\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> ```javascript\n",
      "console.log(\"hello\");\n",
      "```\n",
      "======== Score ======\n",
      "====================\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from textwrap import dedent\n",
    "\n",
    "@task\n",
    "def system_message_solver() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a code generation assistant. Your task is to generate code based on the input provided.\n",
    "    You should ensure that the generated code meets the requirements specified in the input.\n",
    "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
    "    Make sure to output the code and not any additional text or comments.\n",
    "    \"\"\"\n",
    "    \n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            system_message(dedent(SYSTEM_PROMPT)),\n",
    "            generate()\n",
    "        ],\n",
    "    )\n",
    "\n",
    "results = eval(system_message_solver,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://inspect.aisi.org.uk/scorers.html#built-in-scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece6d210312941469f577cdea46f6ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-07T11-43-49+00-00_include-solver_4RrBUeN388sL9nzDmxwNTn.eval\n",
      "======= Sample ========\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: console.log(\n",
      "====================\n",
      "\u001b[33m system     \u001b[39m> You are a code generation assistant. Your task is to generate code based on the input provided.\n",
      "    You should ensure that the generated code meets the requirements specified in the input.\n",
      "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
      "\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Sure! Below is the content for a JavaScript file named `hello-world.js` that prints out \"hello\".\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "You can create a file named `hello-world.js` and copy the above code into it. When you run the file using Node.js or a browser console, it will print \"hello\" to the output.\n",
      "======== Score ======\n",
      "Scorer[includes][VALUE]: C\n",
      "Scorer[includes][EXPLANATION]: Sure! Below is the content for a JavaScript file named `hello-world.js` that prints out \"hello\".\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "You can create a file named `hello-world.js` and copy the above code into it. When you run the file using Node.js or a browser console, it will print \"hello\" to the output.\n",
      "Scorer[match][VALUE]: I\n",
      "Scorer[match][EXPLANATION]: Sure! Below is the content for a JavaScript file named `hello-world.js` that prints out \"hello\".\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "You can create a file named `hello-world.js` and copy the above code into it. When you run the file using Node.js or a browser console, it will print \"hello\" to the output.\n",
      "====================\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from inspect_ai.scorer import includes, match\n",
    "from textwrap import dedent\n",
    "\n",
    "@task\n",
    "def include_solver() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\",\n",
    "            target=\"console.log(\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a code generation assistant. Your task is to generate code based on the input provided.\n",
    "    You should ensure that the generated code meets the requirements specified in the input.\n",
    "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            system_message(dedent(SYSTEM_PROMPT)),\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=[\n",
    "            includes(ignore_case=True), # takes the target and checks if it is included in the generated code\n",
    "            match(location=\"begin\", numeric=False)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "results = eval(include_solver,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFAULT_GRADE_PATTERN:  (?i)GRADE\\s*:\\s*([CPI])(.*)$\n",
      "default_instructions:  \n",
      "After assessing the submitted answer, reply with 'GRADE: $LETTER' (without quotes) where LETTER is one of CI.  Please choose ONE option for the grade: either \"C\" for correct answers, or \"I\" for incorrect answers.\n",
      "\n",
      "For example, after reviewing a correct answer you might write 'GRADE: C' or after reviewing an incorrect answer you might write 'GRADE: I'.\n",
      "\n",
      "First, write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then, end with your answer formatted as 'GRADE: $LETTER' (without quotes) where LETTER is one of CI.\n",
      "\n",
      "DEFAULT_MODEL_GRADED_FACT_TEMPLATE:  \n",
      "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
      "\n",
      "[BEGIN DATA]\n",
      "************\n",
      "[Question]: {question}\n",
      "************\n",
      "[Expert]: {criterion}\n",
      "************\n",
      "[Submission]: {answer}\n",
      "************\n",
      "[END DATA]\n",
      "\n",
      "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
      "\n",
      "Does the submission contain the content in the expert answer?\n",
      "\n",
      "{instructions}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai.scorer._model import DEFAULT_GRADE_PATTERN, DEFAULT_MODEL_GRADED_FACT_TEMPLATE, default_instructions\n",
    "print(\"DEFAULT_GRADE_PATTERN: \",DEFAULT_GRADE_PATTERN)\n",
    "print(\"default_instructions: \",default_instructions(partial_credit=False))\n",
    "print(\"DEFAULT_MODEL_GRADED_FACT_TEMPLATE: \",DEFAULT_MODEL_GRADED_FACT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8d398a48364342951a5023322fa25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFAULT_GRADE_PATTERN (?i)GRADE\\s*:\\s*([CPI])(.*)$\n",
      "default_instructions \n",
      "After assessing the submitted answer, reply with 'GRADE: $LETTER' (without quotes) where LETTER is one of CI.  Please choose ONE option for the grade: either \"C\" for correct answers, or \"I\" for incorrect answers.\n",
      "\n",
      "For example, after reviewing a correct answer you might write 'GRADE: C' or after reviewing an incorrect answer you might write 'GRADE: I'.\n",
      "\n",
      "First, write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then, end with your answer formatted as 'GRADE: $LETTER' (without quotes) where LETTER is one of CI.\n",
      "\n",
      "DEFAULT_MODEL_GRADED_FACT_TEMPLATE \n",
      "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
      "\n",
      "[BEGIN DATA]\n",
      "************\n",
      "[Question]: {question}\n",
      "************\n",
      "[Expert]: {criterion}\n",
      "************\n",
      "[Submission]: {answer}\n",
      "************\n",
      "[END DATA]\n",
      "\n",
      "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
      "\n",
      "Does the submission contain the content in the expert answer?\n",
      "\n",
      "{instructions}\n",
      "\n",
      "Status: success Model: openai/gpt-4o-mini\n",
      "/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-07T11-20-15+00-00_model-graded-fact-solver_8wMq6on5FUsSVTHhHGzGaK.eval\n",
      "======= Sample ========\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: \n",
      "====================\n",
      "\u001b[33m system     \u001b[39m> You are a code generation assistant. Your task is to generate code based on the input provided.\n",
      "    You should ensure that the generated code meets the requirements specified in the input.\n",
      "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
      "\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Here’s the content for a JavaScript file named `hello-world.js` that prints out \"hello\":\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "You can create the file `hello-world.js` and add the above code. When you run this file using Node.js or in a browser's developer console, it will print \"hello\" to the console.\n",
      "======== Score ======\n",
      "Scorer[model_graded_qa][VALUE]: C\n",
      "Scorer[model_graded_qa][EXPLANATION]: To assess the submission based on the given task, I will follow these steps:\n",
      "\n",
      "1. **Understanding the Task**: The task requests the creation of a JavaScript file named `hello-world.js` that prints out \"hello\".\n",
      "\n",
      "2. **Analyzing the Submission**:\n",
      "   - The submission provides the correct content for a JavaScript file, which is to print \"hello\" to the console using `console.log(\"hello\");`.\n",
      "   - The file is correctly named `hello-world.js` as requested.\n",
      "   - Additionally, the submission includes instructions on how to run the code, noting that it can be executed in Node.js or a browser's developer console.\n",
      "\n",
      "3. **Criteria for Evaluation**:\n",
      "   - The submission must meet the requirements of the task: the name of the file, the functionality (printing \"hello\"), and clarity in execution.\n",
      "   - The submission does adhere to all those criteria as it includes the correct file name and functional code that fulfills the task's requirement to print \"hello\".\n",
      "\n",
      "4. **Conclusion**: Since the submission fulfills every aspect of the task clearly and correctly, it meets the criterion set forth.\n",
      "\n",
      "Based on this reasoning, I determine that the submission is correct.\n",
      "\n",
      "GRADE: C\n",
      "====================\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from inspect_ai.scorer import includes, model_graded_qa\n",
    "from textwrap import dedent\n",
    "\n",
    "@task\n",
    "def model_graded_fact_solver() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a code generation assistant. Your task is to generate code based on the input provided.\n",
    "    You should ensure that the generated code meets the requirements specified in the input.\n",
    "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            system_message(dedent(SYSTEM_PROMPT)),\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=[\n",
    "            model_graded_qa() #takes the target as the fact\n",
    "        ]\n",
    "    )\n",
    "\n",
    "results = eval(model_graded_fact_solver,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different grader model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2d95c7be424890813d8cfb0a7355ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-07T11-46-04+00-00_model-graded-fact-solver-o3_hZjDfceuDtoYrMSv83ZH6h.eval\n",
      "======= Sample ========\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: \n",
      "====================\n",
      "\u001b[33m system     \u001b[39m> You are a code generation assistant. Your task is to generate code based on the input provided.\n",
      "    You should ensure that the generated code meets the requirements specified in the input.\n",
      "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
      "\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Here is the content for the `hello-world.js` file that prints out \"hello\":\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "console.log('hello');\n",
      "```\n",
      "\n",
      "Simply create a file named `hello-world.js` and paste the above code into it. When you run this file using Node.js, it will print \"hello\" to the console.\n",
      "======== Score ======\n",
      "Scorer[model_graded_qa][VALUE]: C\n",
      "Scorer[model_graded_qa][EXPLANATION]: Step 1: The task requires generating a file named hello-world.js that prints out \"hello\".  \n",
      "Step 2: The submission provides a code snippet labeled as hello-world.js.  \n",
      "Step 3: The snippet contains the code console.log('hello') which prints out \"hello\" when executed.  \n",
      "Step 4: The explanation also mentions saving the file as hello-world.js and running it using Node.js.  \n",
      "Step 5: All the necessary requirements of the task are met.\n",
      "\n",
      "GRADE: C\n",
      "Scorer[model_graded_qa1][VALUE]: C\n",
      "Scorer[model_graded_qa1][EXPLANATION]: Step-by-step reasoning:\n",
      "\n",
      "1. Understand the task:  \n",
      "   The user needs a JavaScript file named `hello-world.js` whose code prints “hello”.\n",
      "\n",
      "2. Examine the submission:  \n",
      "   The answer provides code:  \n",
      "   ```javascript\n",
      "   // hello-world.js\n",
      "   console.log('hello');\n",
      "   ```  \n",
      "   and instructions for creating the file and running it.\n",
      "\n",
      "3. Compare to the task requirements:  \n",
      "   • File name indicated: yes, `hello-world.js`.  \n",
      "   • Code prints the required text: `console.log('hello');` prints “hello”.  \n",
      "   • No extraneous or missing elements.\n",
      "\n",
      "4. Determine correctness:  \n",
      "   All requirements are satisfied.\n",
      "\n",
      "GRADE: C\n",
      "====================\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from inspect_ai.scorer import includes, model_graded_qa\n",
    "from textwrap import dedent\n",
    "\n",
    "@task\n",
    "def model_graded_fact_solver_o3() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a code generation assistant. Your task is to generate code based on the input provided.\n",
    "    You should ensure that the generated code meets the requirements specified in the input.\n",
    "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            system_message(dedent(SYSTEM_PROMPT)),\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=[\n",
    "            model_graded_qa(model=\"openai/o3-mini\"), #takes the target as the fact\n",
    "            model_graded_qa(model=\"openai/o3\") #takes the target as the fact\n",
    "        ]\n",
    "    )\n",
    "\n",
    "results = eval(model_graded_fact_solver_o3,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fact grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from inspect_ai.scorer import includes, model_graded_fact\n",
    "from textwrap import dedent\n",
    "\n",
    "@task\n",
    "def model_graded_fact_solver() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\",\n",
    "            target=\"The code block should include a console.log statement that prints out hello\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a code generation assistant. Your task is to generate code based on the input provided.\n",
    "    You should ensure that the generated code meets the requirements specified in the input.\n",
    "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            system_message(dedent(SYSTEM_PROMPT)),\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=[\n",
    "            model_graded_fact() #takes the target as the fact\n",
    "        ]\n",
    "    )\n",
    "\n",
    "results = eval(model_graded_fact_solver,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
