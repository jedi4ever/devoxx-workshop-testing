{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect - an Eval Framework\n",
    "## Introduction\n",
    "- As we've seen in the lesson about simple testing, changing the prompt slightly and senting it to the same llm can give different results.\n",
    "- Imagine swapping in and out another model, how do you make sure your code keeps working?\n",
    "- Much like in Test Driven Development, the LLM community talks about *Evals*\n",
    "- Evals can be seen as a test suite to check the results across multiple prompts and llms\n",
    "- We've seen how it works, but luckily there exist frameworks that have some helpers , so we don't have to do it all by ourselves\n",
    "\n",
    "The framework we will show here is `Inspect AI` - <https://inspect.ai-safety-institute.org.uk/>. It comes in the form of a VSCode plugin too, this is eanbled in this workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : we are pinning the inspect_ai due to a recent breaking change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai anthropic ipywidgets colorama\n",
    "import os\n",
    "os.environ['XDG_RUNTIME_DIR'] = \"/tmp\"\n",
    "\n",
    "from helpers.reporter.pretty import pretty_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your first solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6567c43e717f4b238aff0a8baf85a410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-07T12-52-13+00-00_simple-generate_N6PWPz8u4tEVxZnwikVr79.eval\n",
      "======= Sample ========\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: \n",
      "====================\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Certainly! Below is the content for a JavaScript file named `hello-world.js` that prints out \"hello\".\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "You can create a file named `hello-world.js` and copy the above code into it. When you run this file with Node.js (or in the browser's console), it will output \"hello\". \n",
      "\n",
      "To run it using Node.js, you would navigate to the directory where the file is located in your terminal and run the following command:\n",
      "\n",
      "```bash\n",
      "node hello-world.js\n",
      "```\n",
      "======== Score ======\n",
      "====================\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import generate\n",
    "\n",
    "@task\n",
    "def simple_generate() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            generate(model = \"openai/gpt-4o-mini\",)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "results = eval(simple_generate, max_steps=10,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5b1cc21e8d46fcbdd98842ab2a5750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-07T12-53-05+00-00_simple-generate_HREVmyWUxcPtsR3fkJAk97.eval\n",
      "======= Sample ========\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: \n",
      "====================\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Here's a simple `hello-world.js` file that prints \"hello\" to the console. You can create this file in your text editor and run it using Node.js or directly in a browser console.\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "To execute this file:\n",
      "\n",
      "1. Save the above code in a file named `hello-world.js`.\n",
      "2. Open your terminal (command prompt) and navigate to the directory where the file is located.\n",
      "3. Run the following command:\n",
      "\n",
      "```bash\n",
      "node hello-world.js\n",
      "```\n",
      "\n",
      "This should print \"hello\" in the console. If you run this in a web browser console, simply copy the `console.log(\"hello\");` line and paste it there to see the output.\n",
      "======== Score ======\n",
      "====================\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Set default model to gpt-4o-mini\n",
    "os.environ['INSPECT_EVAL_MODEL'] = \"openai/gpt-4o-mini\"\n",
    "\n",
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import generate\n",
    "\n",
    "@task\n",
    "def simple_generate() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            generate()\n",
    "        ],\n",
    "    )\n",
    "\n",
    "results = eval(simple_generate,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2b79779a4b4217b8ad9be30db916c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "Status: success Model: openai/gpt-4o\n",
      "/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-07T13-05-19+00-00_simple-generate_HxSyMQZoMT2iEPEt3Tzjd7.eval\n",
      "======= Sample ========\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: \n",
      "====================\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Here is a simple JavaScript code snippet that will print \"Hello\" to the console. You can save this code in a file named `hello-world.js`.\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "console.log(\"Hello\");\n",
      "```\n",
      "\n",
      "To run this script, you can use Node.js. Simply follow these steps:\n",
      "\n",
      "1. Make sure you have Node.js installed on your machine.\n",
      "2. Create a new file named `hello-world.js`.\n",
      "3. Copy and paste the above code into `hello-world.js`.\n",
      "4. Open your terminal or command prompt.\n",
      "5. Navigate to the directory where you saved the file.\n",
      "6. Run the command: \n",
      "\n",
      "```bash\n",
      "node hello-world.js\n",
      "```\n",
      "\n",
      "You should see the output:\n",
      "\n",
      "```\n",
      "Hello\n",
      "```\n",
      "======== Score ======\n",
      "====================\n",
      "********************************************************************************\n",
      "/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-07T13-05-19+00-00_simple-generate_Qm84fRq2hqMqpys87eegU5.eval\n",
      "======= Sample ========\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: \n",
      "====================\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Certainly! Here is an example of a JavaScript file named `hello-world.js` which prints out \"Hello\":\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "\n",
      "console.log(\"Hello\");\n",
      "```\n",
      "\n",
      "To execute this script, you can use a JavaScript runtime like Node.js. Save the above code in a file named `hello-world.js`, open your terminal or command prompt, navigate to the directory where the file is located, and run the following command:\n",
      "\n",
      "```bash\n",
      "node hello-world.js\n",
      "```\n",
      "\n",
      "This will output:\n",
      "\n",
      "```\n",
      "Hello\n",
      "```\n",
      "======== Score ======\n",
      "====================\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Set default model to gpt-4o-mini\n",
    "os.environ['INSPECT_EVAL_MODEL'] = \"openai/gpt-4o-mini\"\n",
    "\n",
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import generate\n",
    "\n",
    "@task\n",
    "def simple_generate() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            generate()\n",
    "        ],\n",
    "    )\n",
    "\n",
    "models = [\n",
    "    \"openai/gpt-4o-mini\",\n",
    "    \"openai/gpt-4o\",\n",
    "]\n",
    "results = eval(simple_generate,log_level=\"info\", model = models, display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9c0f535a5a48ce9753439672d2a91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-07T10-52-41+00-00_system-message-solver_Sps8AtzSdJA8xvyXeuqhYv.eval\n",
      "======= Sample ========\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: \n",
      "====================\n",
      "\u001b[33m system     \u001b[39m> You are a code generation assistant. Your task is to generate code based on the input provided.\n",
      "    You should ensure that the generated code meets the requirements specified in the input.\n",
      "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
      "    Make sure to output the code and not any additional text or comments.\n",
      "\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> ```javascript\n",
      "console.log(\"hello\");\n",
      "```\n",
      "======== Score ======\n",
      "====================\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from textwrap import dedent\n",
    "\n",
    "@task\n",
    "def system_message_solver() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a code generation assistant. Your task is to generate code based on the input provided.\n",
    "    You should ensure that the generated code meets the requirements specified in the input.\n",
    "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
    "    Make sure to output the code and not any additional text or comments.\n",
    "    \"\"\"\n",
    "    \n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            system_message(dedent(SYSTEM_PROMPT)),\n",
    "            generate()\n",
    "        ],\n",
    "    )\n",
    "\n",
    "results = eval(system_message_solver,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://inspect.aisi.org.uk/scorers.html#built-in-scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece6d210312941469f577cdea46f6ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-07T11-43-49+00-00_include-solver_4RrBUeN388sL9nzDmxwNTn.eval\n",
      "======= Sample ========\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: console.log(\n",
      "====================\n",
      "\u001b[33m system     \u001b[39m> You are a code generation assistant. Your task is to generate code based on the input provided.\n",
      "    You should ensure that the generated code meets the requirements specified in the input.\n",
      "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
      "\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Sure! Below is the content for a JavaScript file named `hello-world.js` that prints out \"hello\".\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "You can create a file named `hello-world.js` and copy the above code into it. When you run the file using Node.js or a browser console, it will print \"hello\" to the output.\n",
      "======== Score ======\n",
      "Scorer[includes][VALUE]: C\n",
      "Scorer[includes][EXPLANATION]: Sure! Below is the content for a JavaScript file named `hello-world.js` that prints out \"hello\".\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "You can create a file named `hello-world.js` and copy the above code into it. When you run the file using Node.js or a browser console, it will print \"hello\" to the output.\n",
      "Scorer[match][VALUE]: I\n",
      "Scorer[match][EXPLANATION]: Sure! Below is the content for a JavaScript file named `hello-world.js` that prints out \"hello\".\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "You can create a file named `hello-world.js` and copy the above code into it. When you run the file using Node.js or a browser console, it will print \"hello\" to the output.\n",
      "====================\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from inspect_ai.scorer import includes, match\n",
    "from textwrap import dedent\n",
    "\n",
    "@task\n",
    "def include_solver() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\",\n",
    "            target=\"console.log(\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a code generation assistant. Your task is to generate code based on the input provided.\n",
    "    You should ensure that the generated code meets the requirements specified in the input.\n",
    "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            system_message(dedent(SYSTEM_PROMPT)),\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=[\n",
    "            includes(ignore_case=True), # takes the target and checks if it is included in the generated code\n",
    "            match(location=\"begin\", numeric=False)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "results = eval(include_solver,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFAULT_GRADE_PATTERN:  (?i)GRADE\\s*:\\s*([CPI])(.*)$\n",
      "default_instructions:  \n",
      "After assessing the submitted answer, reply with 'GRADE: $LETTER' (without quotes) where LETTER is one of CI.  Please choose ONE option for the grade: either \"C\" for correct answers, or \"I\" for incorrect answers.\n",
      "\n",
      "For example, after reviewing a correct answer you might write 'GRADE: C' or after reviewing an incorrect answer you might write 'GRADE: I'.\n",
      "\n",
      "First, write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then, end with your answer formatted as 'GRADE: $LETTER' (without quotes) where LETTER is one of CI.\n",
      "\n",
      "DEFAULT_MODEL_GRADED_FACT_TEMPLATE:  \n",
      "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
      "\n",
      "[BEGIN DATA]\n",
      "************\n",
      "[Question]: {question}\n",
      "************\n",
      "[Expert]: {criterion}\n",
      "************\n",
      "[Submission]: {answer}\n",
      "************\n",
      "[END DATA]\n",
      "\n",
      "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
      "\n",
      "Does the submission contain the content in the expert answer?\n",
      "\n",
      "{instructions}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai.scorer._model import DEFAULT_GRADE_PATTERN, DEFAULT_MODEL_GRADED_FACT_TEMPLATE, default_instructions\n",
    "print(\"DEFAULT_GRADE_PATTERN: \",DEFAULT_GRADE_PATTERN)\n",
    "print(\"default_instructions: \",default_instructions(partial_credit=False))\n",
    "print(\"DEFAULT_MODEL_GRADED_FACT_TEMPLATE: \",DEFAULT_MODEL_GRADED_FACT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8d398a48364342951a5023322fa25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFAULT_GRADE_PATTERN (?i)GRADE\\s*:\\s*([CPI])(.*)$\n",
      "default_instructions \n",
      "After assessing the submitted answer, reply with 'GRADE: $LETTER' (without quotes) where LETTER is one of CI.  Please choose ONE option for the grade: either \"C\" for correct answers, or \"I\" for incorrect answers.\n",
      "\n",
      "For example, after reviewing a correct answer you might write 'GRADE: C' or after reviewing an incorrect answer you might write 'GRADE: I'.\n",
      "\n",
      "First, write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then, end with your answer formatted as 'GRADE: $LETTER' (without quotes) where LETTER is one of CI.\n",
      "\n",
      "DEFAULT_MODEL_GRADED_FACT_TEMPLATE \n",
      "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
      "\n",
      "[BEGIN DATA]\n",
      "************\n",
      "[Question]: {question}\n",
      "************\n",
      "[Expert]: {criterion}\n",
      "************\n",
      "[Submission]: {answer}\n",
      "************\n",
      "[END DATA]\n",
      "\n",
      "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
      "\n",
      "Does the submission contain the content in the expert answer?\n",
      "\n",
      "{instructions}\n",
      "\n",
      "Status: success Model: openai/gpt-4o-mini\n",
      "/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-07T11-20-15+00-00_model-graded-fact-solver_8wMq6on5FUsSVTHhHGzGaK.eval\n",
      "======= Sample ========\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: \n",
      "====================\n",
      "\u001b[33m system     \u001b[39m> You are a code generation assistant. Your task is to generate code based on the input provided.\n",
      "    You should ensure that the generated code meets the requirements specified in the input.\n",
      "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
      "\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Here’s the content for a JavaScript file named `hello-world.js` that prints out \"hello\":\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "You can create the file `hello-world.js` and add the above code. When you run this file using Node.js or in a browser's developer console, it will print \"hello\" to the console.\n",
      "======== Score ======\n",
      "Scorer[model_graded_qa][VALUE]: C\n",
      "Scorer[model_graded_qa][EXPLANATION]: To assess the submission based on the given task, I will follow these steps:\n",
      "\n",
      "1. **Understanding the Task**: The task requests the creation of a JavaScript file named `hello-world.js` that prints out \"hello\".\n",
      "\n",
      "2. **Analyzing the Submission**:\n",
      "   - The submission provides the correct content for a JavaScript file, which is to print \"hello\" to the console using `console.log(\"hello\");`.\n",
      "   - The file is correctly named `hello-world.js` as requested.\n",
      "   - Additionally, the submission includes instructions on how to run the code, noting that it can be executed in Node.js or a browser's developer console.\n",
      "\n",
      "3. **Criteria for Evaluation**:\n",
      "   - The submission must meet the requirements of the task: the name of the file, the functionality (printing \"hello\"), and clarity in execution.\n",
      "   - The submission does adhere to all those criteria as it includes the correct file name and functional code that fulfills the task's requirement to print \"hello\".\n",
      "\n",
      "4. **Conclusion**: Since the submission fulfills every aspect of the task clearly and correctly, it meets the criterion set forth.\n",
      "\n",
      "Based on this reasoning, I determine that the submission is correct.\n",
      "\n",
      "GRADE: C\n",
      "====================\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from inspect_ai.scorer import includes, model_graded_qa\n",
    "from textwrap import dedent\n",
    "\n",
    "@task\n",
    "def model_graded_fact_solver() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a code generation assistant. Your task is to generate code based on the input provided.\n",
    "    You should ensure that the generated code meets the requirements specified in the input.\n",
    "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            system_message(dedent(SYSTEM_PROMPT)),\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=[\n",
    "            model_graded_qa() #takes the target as the fact\n",
    "        ]\n",
    "    )\n",
    "\n",
    "results = eval(model_graded_fact_solver,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different grader model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2d95c7be424890813d8cfb0a7355ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-07T11-46-04+00-00_model-graded-fact-solver-o3_hZjDfceuDtoYrMSv83ZH6h.eval\n",
      "======= Sample ========\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: \n",
      "====================\n",
      "\u001b[33m system     \u001b[39m> You are a code generation assistant. Your task is to generate code based on the input provided.\n",
      "    You should ensure that the generated code meets the requirements specified in the input.\n",
      "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
      "\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Here is the content for the `hello-world.js` file that prints out \"hello\":\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "console.log('hello');\n",
      "```\n",
      "\n",
      "Simply create a file named `hello-world.js` and paste the above code into it. When you run this file using Node.js, it will print \"hello\" to the console.\n",
      "======== Score ======\n",
      "Scorer[model_graded_qa][VALUE]: C\n",
      "Scorer[model_graded_qa][EXPLANATION]: Step 1: The task requires generating a file named hello-world.js that prints out \"hello\".  \n",
      "Step 2: The submission provides a code snippet labeled as hello-world.js.  \n",
      "Step 3: The snippet contains the code console.log('hello') which prints out \"hello\" when executed.  \n",
      "Step 4: The explanation also mentions saving the file as hello-world.js and running it using Node.js.  \n",
      "Step 5: All the necessary requirements of the task are met.\n",
      "\n",
      "GRADE: C\n",
      "Scorer[model_graded_qa1][VALUE]: C\n",
      "Scorer[model_graded_qa1][EXPLANATION]: Step-by-step reasoning:\n",
      "\n",
      "1. Understand the task:  \n",
      "   The user needs a JavaScript file named `hello-world.js` whose code prints “hello”.\n",
      "\n",
      "2. Examine the submission:  \n",
      "   The answer provides code:  \n",
      "   ```javascript\n",
      "   // hello-world.js\n",
      "   console.log('hello');\n",
      "   ```  \n",
      "   and instructions for creating the file and running it.\n",
      "\n",
      "3. Compare to the task requirements:  \n",
      "   • File name indicated: yes, `hello-world.js`.  \n",
      "   • Code prints the required text: `console.log('hello');` prints “hello”.  \n",
      "   • No extraneous or missing elements.\n",
      "\n",
      "4. Determine correctness:  \n",
      "   All requirements are satisfied.\n",
      "\n",
      "GRADE: C\n",
      "====================\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from inspect_ai.scorer import includes, model_graded_qa\n",
    "from textwrap import dedent\n",
    "\n",
    "@task\n",
    "def model_graded_fact_solver_o3() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a code generation assistant. Your task is to generate code based on the input provided.\n",
    "    You should ensure that the generated code meets the requirements specified in the input.\n",
    "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            system_message(dedent(SYSTEM_PROMPT)),\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=[\n",
    "            model_graded_qa(model=\"openai/o3-mini\"), #takes the target as the fact\n",
    "            model_graded_qa(model=\"openai/o3\") #takes the target as the fact\n",
    "        ]\n",
    "    )\n",
    "\n",
    "results = eval(model_graded_fact_solver_o3,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fact grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from inspect_ai.scorer import includes, model_graded_fact\n",
    "from textwrap import dedent\n",
    "\n",
    "@task\n",
    "def model_graded_fact_solver() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\",\n",
    "            target=\"The code block should include a console.log statement that prints out hello\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a code generation assistant. Your task is to generate code based on the input provided.\n",
    "    You should ensure that the generated code meets the requirements specified in the input.\n",
    "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            system_message(dedent(SYSTEM_PROMPT)),\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=[\n",
    "            model_graded_fact() #takes the target as the fact\n",
    "        ]\n",
    "    )\n",
    "\n",
    "results = eval(model_graded_fact_solver,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b085b06bfac4ac69908563ac825cd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-07T14-21-49+00-00_picture-basic_mS3RevET4bxFuzvckrmZKt.eval\n",
      "======= Sample ========\n",
      "input : [ChatMessageUser(id='JH5Lx8YjrzXQX6z88uJekT', content=[ContentText(type='text', text='(Image)', refusal=None), ContentText(type='text', text='What is this a picture of?', refusal=None)], source=None, internal=None, role='user', tool_call_id=None)]\n",
      "target: The picture represents the homepage of Uber.\n",
      "====================\n",
      "\n",
      "\u001b[33m user       \u001b[39m+> What is this a picture of?\n",
      "\u001b[33m assistant  \u001b[39m> This is a screenshot of the Uber website, showcasing their ride-hailing service. It features sections for entering pickup and drop-off locations, options to select the date and time, a map on the right side, and a notification about cookie use.\n",
      "======== Score ======\n",
      "Scorer[model_graded_fact][VALUE]: C\n",
      "Scorer[model_graded_fact][EXPLANATION]: To determine whether the submitted answer contains the content in the expert answer, I will analyze both responses carefully.\n",
      "\n",
      "1. **Expert's Response**: \"The picture represents the homepage of Uber.\"\n",
      "   - This statement notes that the image is the homepage of Uber, which directly indicates the type of content (i.e., related to Uber).\n",
      "\n",
      "2. **Submitted Answer**: \"This is a screenshot of the Uber website, showcasing their ride-hailing service. It features sections for entering pickup and drop-off locations, options to select the date and time, a map on the right side, and a notification about cookie use.\"\n",
      "   - This submission describes a screenshot of the Uber website and elaborates on various features present on that website (like entering pickup/drop-off locations, time selection, and a map), which are all relevant elements of the Uber service.\n",
      "\n",
      "3. **Comparison**:\n",
      "   - Both answers reference \"Uber\" and its online presence. The expert identifies it simply as the \"homepage of Uber,\" while the submission is more detailed, providing information about the specific features on that homepage (ride-hailing service, sections for inputting locations, options, map, cookie notification).\n",
      "   - Importantly, the term \"homepage\" implicitly encompasses what the submission describes—it's not just a generic screenshot; it details functionalities that reinforce the identification of the image.\n",
      "\n",
      "4. **Conclusion**: \n",
      "   - The submitted answer fully describes the content indicated by the expert's answer. It is not just a homepage, but the features within that context validate the identification.\n",
      "   - Since the submitted answer does contain the essence of what the expert stated, it qualifies as factually correct in terms of content representation.\n",
      "\n",
      "Following this reasoning, I conclude:\n",
      "\n",
      "GRADE: C\n",
      "====================\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.model import ChatMessageUser, ContentImage, ContentText\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from inspect_ai.scorer import includes, model_graded_fact\n",
    "\n",
    "@task\n",
    "def picture_basic():\n",
    "    dataset = [\n",
    "        Sample(input = [\n",
    "            ChatMessageUser(content = [\n",
    "                ContentImage(image=\"data/screenshot.png\"),\n",
    "                ContentText(text=\"What is this a picture of?\")\n",
    "            ])\n",
    "        ], target=\"The picture represents the homepage of Uber.\")\n",
    "    ]\n",
    "\n",
    "    task = Task(\n",
    "        dataset=dataset,\n",
    "        solver=generate(),\n",
    "        scorer=model_graded_fact(),\n",
    "    )\n",
    "    return task\n",
    "\n",
    "\n",
    "results = eval(picture_basic,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
