{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorers\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai anthropic ipywidgets colorama\n",
    "import os\n",
    "os.environ['XDG_RUNTIME_DIR'] = \"/tmp\"\n",
    "\n",
    "from helpers.reporter.pretty import pretty_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorer\n",
    "- Once the solver has done it's work, we want to check the results\n",
    "- A `scorer` takes the output after al the `solvers` are finished.\n",
    "- Inspect.AI has mnany solvers integrated <https://inspect.aisi.org.uk/scorers.html#built-in-scorers>\n",
    "- here we used `included` and `match` to check the output of the generate solver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0b347ed765471d8e36178444aedc51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-08T07-12-43+00-00_include-solver_jPj6kGhoARgQVSH3kB9EKD.eval\">/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-08T07-12-43+00-00_include-solver_jPj6kGhoARgQVSH3kB9EKD.eval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "======= Sample Begin =================================================================\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: console.log(\n",
      "======================================================================================\n",
      "\u001b[33m system     \u001b[39m> You are a code generation assistant. Your task is to generate code based on the input provided.\n",
      "    You should ensure that the generated code meets the requirements specified in the input.\n",
      "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
      "\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Here is the content for a JavaScript file named `hello-world.js` that prints out \"hello\":\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "You can create this file by copying the above code into a text editor and saving it with the name `hello-world.js`. When you run this file using Node.js or in a browser console, it will print \"hello\" to the output.\n",
      "======== Score =======================================================================\n",
      "Scorer[includes][VALUE]: C\n",
      "Scorer[includes][EXPLANATION]: Here is the content for a JavaScript file named `hello-world.js` that prints out \"hello\":\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "You can create this file by copying the above code into a text editor and saving it with the name `hello-world.js`. When you run this file using Node.js or in a browser console, it will print \"hello\" to the output.\n",
      "Scorer[match][VALUE]: I\n",
      "Scorer[match][EXPLANATION]: Here is the content for a JavaScript file named `hello-world.js` that prints out \"hello\":\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "You can create this file by copying the above code into a text editor and saving it with the name `hello-world.js`. When you run this file using Node.js or in a browser console, it will print \"hello\" to the output.\n",
      "======================================================================================\n",
      "**** End Sample ******************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from inspect_ai.scorer import includes, match\n",
    "from textwrap import dedent\n",
    "\n",
    "@task\n",
    "def include_solver() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\",\n",
    "            target=\"console.log(\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a code generation assistant. Your task is to generate code based on the input provided.\n",
    "    You should ensure that the generated code meets the requirements specified in the input.\n",
    "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            system_message(dedent(SYSTEM_PROMPT)),\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=[\n",
    "            includes(ignore_case=True), # takes the target and checks if it is included in the generated code\n",
    "            match(location=\"begin\", numeric=False)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "results = eval(include_solver,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model QA Grader\n",
    "- We can also ask an LLM to grade the result it generated.\n",
    "- This is the `model_qa_greader` solver : it looks at the output of the solvers and then does an assessment given the initial ask in the Samnple input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca33fac51f0e42be9f8ccc16dd3a31e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-08T07-12-47+00-00_model-graded-solver_7NU7RkBzaSoceEWuTo8CSB.eval\">/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-08T07-12-47+00-00_model-graded-solver_7NU7RkBzaSoceEWuTo8CSB.eval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "======= Sample Begin =================================================================\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: \n",
      "======================================================================================\n",
      "\u001b[33m system     \u001b[39m> You are a code generation assistant. Your task is to generate code based on the input provided.\n",
      "    You should ensure that the generated code meets the requirements specified in the input.\n",
      "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
      "\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Here is the content for the `hello-world.js` file that prints out \"hello\":\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "Make sure to create a file named `hello-world.js` and place the above code inside it. You can run this JavaScript file using Node.js with the command:\n",
      "\n",
      "```\n",
      "node hello-world.js\n",
      "``` \n",
      "\n",
      "This will print \"hello\" to the console.\n",
      "======== Score =======================================================================\n",
      "Scorer[model_graded_qa][VALUE]: C\n",
      "Scorer[model_graded_qa][EXPLANATION]: To assess the submission against the task, I'll evaluate the key requirements of the task and see if the submitted answer meets those requirements.\n",
      "\n",
      "1. **Task Requirement**: The task specifically asks to generate a JavaScript file named `hello-world.js` that prints out \"hello\".\n",
      "\n",
      "2. **Examination of Submission**:\n",
      "   - The submission begins by stating that it will provide the content for the `hello-world.js` file.\n",
      "   - It correctly includes the JavaScript code: `console.log(\"hello\");` which will print \"hello\" to the console.\n",
      "   - The submission confirms that the file should be named `hello-world.js`, which matches the requirement.\n",
      "   - Furthermore, the response includes instruction on how to run the file using Node.js, which is useful for those who might not know how to execute JavaScript files in a server-side environment.\n",
      "\n",
      "3. **Evaluation of Completeness**: \n",
      "   - The submission provides not only the content of the file but also confirmation of the file name and execution instructions. This is comprehensive guidance that goes beyond merely supplying code.\n",
      "   - There are no omissions or errors in the provided code or instructions.\n",
      "\n",
      "Given these points, I conclude that the submission meets all aspects of the task's criterion satisfactorily.\n",
      "\n",
      "Therefore, I will grade this submission as a correct answer.\n",
      "\n",
      "GRADE: C\n",
      "======================================================================================\n",
      "**** End Sample ******************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from inspect_ai.scorer import includes, model_graded_qa\n",
    "from textwrap import dedent\n",
    "\n",
    "@task\n",
    "def model_graded_solver() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a code generation assistant. Your task is to generate code based on the input provided.\n",
    "    You should ensure that the generated code meets the requirements specified in the input.\n",
    "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            system_message(dedent(SYSTEM_PROMPT)),\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=[\n",
    "            model_graded_qa()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "results = eval(model_graded_solver,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see the prompt template for it.\n",
    "- The `question` is the oringal input in the Sample\n",
    "- The `criterion` and `answer` are the result of all the solvers.\n",
    "- So it judges what it created based on the original question.\n",
    "- Internally it used a grading mechanism to have the scorer grade the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFAULT_GRADE_PATTERN:  (?i)GRADE\\s*:\\s*([CPI])(.*)$\n",
      "DEFAULT_MODEL_GRADED_QA_TEMPLATE:  \n",
      "You are assessing a submitted answer on a given task based on a criterion. Here is the data:\n",
      "\n",
      "[BEGIN DATA]\n",
      "***\n",
      "[Task]: {question}\n",
      "***\n",
      "[Submission]: {answer}\n",
      "***\n",
      "[Criterion]: {criterion}\n",
      "***\n",
      "[END DATA]\n",
      "\n",
      "Does the submission meet the criterion?\n",
      "\n",
      "{instructions}\n",
      "\n",
      "default_instructions:  \n",
      "After assessing the submitted answer, reply with 'GRADE: $LETTER' (without quotes) where LETTER is one of CI.  Please choose ONE option for the grade: either \"C\" for correct answers, or \"I\" for incorrect answers.\n",
      "\n",
      "For example, after reviewing a correct answer you might write 'GRADE: C' or after reviewing an incorrect answer you might write 'GRADE: I'.\n",
      "\n",
      "First, write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then, end with your answer formatted as 'GRADE: $LETTER' (without quotes) where LETTER is one of CI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai.scorer._model import DEFAULT_GRADE_PATTERN, DEFAULT_MODEL_GRADED_QA_TEMPLATE, default_instructions\n",
    "print(\"DEFAULT_GRADE_PATTERN: \",DEFAULT_GRADE_PATTERN)\n",
    "print(\"DEFAULT_MODEL_GRADED_QA_TEMPLATE: \",DEFAULT_MODEL_GRADED_QA_TEMPLATE)\n",
    "print(\"default_instructions: \",default_instructions(partial_credit=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different grader model\n",
    "- You can also specify a different model to judge the other model that was used to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18a58a36c654ebda466a0d405dd3ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-08T07-12-55+00-00_model-graded-solver-o3_FhyZQ4cYxW74XNvKEbPsMx.eval\">/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-08T07-12-55+00-00_model-graded-solver-o3_FhyZQ4cYxW74XNvKEbPsMx.eval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "======= Sample Begin =================================================================\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: \n",
      "======================================================================================\n",
      "\u001b[33m system     \u001b[39m> You are a code generation assistant. Your task is to generate code based on the input provided.\n",
      "    You should ensure that the generated code meets the requirements specified in the input.\n",
      "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
      "\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Here is the code for a JavaScript file named `hello-world.js` that prints out \"hello\":\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "To create the file, you can simply copy the code above into a file named `hello-world.js`. When you run this file using Node.js or in a browser console, it will print \"hello\" to the output.\n",
      "======== Score =======================================================================\n",
      "Scorer[model_graded_qa][VALUE]: C\n",
      "Scorer[model_graded_qa][EXPLANATION]: Step 1: The task requires a JavaScript file named \"hello-world.js\".  \n",
      "Step 2: The file should print out \"hello\".  \n",
      "Step 3: The submission provides a JavaScript file with the code \"console.log(\"hello\");\", which correctly prints \"hello\".  \n",
      "Step 4: The naming of the file and the code content satisfy the task.  \n",
      "\n",
      "GRADE: C\n",
      "Scorer[model_graded_qa1][VALUE]: C\n",
      "Scorer[model_graded_qa1][EXPLANATION]: Step-by-step reasoning:\n",
      "\n",
      "1. Task requirement  \n",
      "   • A JavaScript file named “hello-world.js” must be generated.  \n",
      "   • Its behavior should be to print out the word “hello”.\n",
      "\n",
      "2. Submitted answer  \n",
      "   • Supplies code snippet:  \n",
      "     ```javascript\n",
      "     // hello-world.js\n",
      "     console.log(\"hello\");\n",
      "     ```  \n",
      "   • Explains that saving this snippet as hello-world.js and running it will print “hello”.\n",
      "\n",
      "3. Comparison  \n",
      "   • The filename indicated matches the task requirement.  \n",
      "   • The code uses console.log(\"hello\"); which prints exactly the string “hello”.  \n",
      "   • No contradictions or omissions.\n",
      "\n",
      "4. Conclusion  \n",
      "   • Submission satisfies all aspects of the task.\n",
      "\n",
      "GRADE: C\n",
      "======================================================================================\n",
      "**** End Sample ******************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from inspect_ai.scorer import includes, model_graded_qa\n",
    "from textwrap import dedent\n",
    "\n",
    "@task\n",
    "def model_graded_solver_o3() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a code generation assistant. Your task is to generate code based on the input provided.\n",
    "    You should ensure that the generated code meets the requirements specified in the input.\n",
    "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            system_message(dedent(SYSTEM_PROMPT)),\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=[\n",
    "            model_graded_qa(model=\"openai/o3-mini\"), #takes the target as the fact\n",
    "            model_graded_qa(model=\"openai/o3\") #takes the target as the fact\n",
    "        ]\n",
    "    )\n",
    "\n",
    "results = eval(model_graded_solver_o3,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fact grader\n",
    "- Now instead of just using the initial question to judge the results, we can set a target in the sample.\n",
    "- The qa judge will now take that as `expert` input and seed if that fact is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3d1c9605fc400a8648eb303d4f25d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-08T07-13-12+00-00_model-graded-fact-solver_mfN3xjDawzz5fSEBP34fdg.eval\">/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-08T07-13-12+00-00_model-graded-fact-solver_mfN3xjDawzz5fSEBP34fdg.eval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "======= Sample Begin =================================================================\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: The code block should include a console.log statement that prints out hello\n",
      "======================================================================================\n",
      "\u001b[33m system     \u001b[39m> You are a code generation assistant. Your task is to generate code based on the input provided.\n",
      "    You should ensure that the generated code meets the requirements specified in the input.\n",
      "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
      "\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Here is a simple JavaScript file named `hello-world.js` that prints out \"hello\":\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "You can create a file named `hello-world.js` and copy the above code into it. Running this file will output \"hello\" to the console.\n",
      "======== Score =======================================================================\n",
      "Scorer[model_graded_fact][VALUE]: C\n",
      "Scorer[model_graded_fact][EXPLANATION]: To determine if the submitted answer contains the essential content found in the expert answer, I will follow these steps:\n",
      "\n",
      "1. **Identify the expert answer's key content**: The expert states that the code block should include a `console.log` statement that prints out \"hello\". The essence of this answer is that it specifies a necessary component of the JavaScript code, which is the use of `console.log` to print \"hello\". \n",
      "\n",
      "2. **Analyze the submitted answer**: The submission presents a code block that:\n",
      "   - Clearly indicates the name of the file as `hello-world.js`.\n",
      "   - Contains the `console.log(\"hello\");` line which is necessary for printing the message \"hello\".\n",
      "   - Additionally, it provides an explanation that instructs how to use the code.\n",
      "\n",
      "3. **Check for alignment**: The submission includes all necessary parts of the expert's advice:\n",
      "   - It contains the `console.log` command within the code.\n",
      "   - The string being printed is \"hello\", which aligns perfectly with the expert's indication of what should be output.\n",
      "\n",
      "4. **Conclusion**: Given that the submission contains the `console.log` statement exactly as the expert advised, it matches the expert's answer in terms of factual content.\n",
      "\n",
      "As a result of this analysis, the submitted answer meets the requirements set by the expert answer.\n",
      "\n",
      "Therefore, the grade is: GRADE: C\n",
      "======================================================================================\n",
      "**** End Sample ******************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from inspect_ai.scorer import includes, model_graded_fact\n",
    "from textwrap import dedent\n",
    "\n",
    "@task\n",
    "def model_graded_fact_solver() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\",\n",
    "            target=\"The code block should include a console.log statement that prints out hello\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a code generation assistant. Your task is to generate code based on the input provided.\n",
    "    You should ensure that the generated code meets the requirements specified in the input.\n",
    "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            system_message(dedent(SYSTEM_PROMPT)),\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=[\n",
    "            model_graded_fact() #takes the target as the fact\n",
    "        ]\n",
    "    )\n",
    "\n",
    "results = eval(model_graded_fact_solver,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Judge scoring\n",
    "- `model_graded_qa_fact` only takes input from the Sample.\n",
    "- We can use a simple helper function to replace the prompt with our own criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
      "\n",
      "[BEGIN DATA]\n",
      "************\n",
      "[Question]: {question}\n",
      "************\n",
      "[Expert]: The code block should include a console.log statement\n",
      "************\n",
      "[Submission]: {answer}\n",
      "************\n",
      "[END DATA]\n",
      "\n",
      "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
      "\n",
      "Does the submission contain the content in the expert answer?\n",
      "\n",
      "{instructions}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai.scorer._model import DEFAULT_MODEL_GRADED_FACT_TEMPLATE\n",
    "\n",
    "def judge_criteria(criteria):\n",
    "    return DEFAULT_MODEL_GRADED_FACT_TEMPLATE.replace(\"{criterion}\", criteria)\n",
    "\n",
    "print(judge_criteria(\"The code block should include a console.log statement\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b7512ad250483986e668e7118f3104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-08T07-13-19+00-00_judge-solver_NhzZ9TytTfiKb9WFvbtorU.eval\">/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-08T07-13-19+00-00_judge-solver_NhzZ9TytTfiKb9WFvbtorU.eval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "======= Sample Begin =================================================================\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: The code block should include a console.log statement that prints out hello\n",
      "======================================================================================\n",
      "\u001b[33m system     \u001b[39m> You are a code generation assistant. Your task is to generate code based on the input provided.\n",
      "    You should ensure that the generated code meets the requirements specified in the input.\n",
      "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
      "\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Sure! Here is the content for a JavaScript file named `hello-world.js` that prints out \"hello\":\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "console.log('hello');\n",
      "```\n",
      "\n",
      "You can create the file named `hello-world.js` and add the above code to it. When you run this script using Node.js or in a browser console, it will output \"hello\".\n",
      "======== Score =======================================================================\n",
      "Scorer[model_graded_fact][VALUE]: C\n",
      "Scorer[model_graded_fact][EXPLANATION]: To assess whether the submitted answer contains the content in the expert answer, I will follow these steps:\n",
      "\n",
      "1. **Identify Key Points in the Expert Answer**: The expert answer states that \"the code block should include a console.log statement.\" This emphasizes the necessity of including such a statement in the JavaScript code to meet the requirement of printing out \"hello\".\n",
      "\n",
      "2. **Examine the Submitted Answer**: The submission provides the name of the JavaScript file, `hello-world.js`, and clearly includes the line of code:\n",
      "   ```javascript\n",
      "   console.log('hello');\n",
      "   ```\n",
      "   This line is an implementation of a console log statement that fulfills the requirement to print \"hello\".\n",
      "\n",
      "3. **Confirm the Inclusion of Expert Recommendations**: The submitted answer not only includes a `console.log` statement but also correctly prints out \"hello\", which is aligned with the expert’s requirement. Additionally, it describes how to use the created file, suggesting it can be run with Node.js or in a browser console.\n",
      "\n",
      "4. **Evaluate Completeness and Accuracy**: The submission captures all essential elements indicated by the expert. It contains the `console.log` within the required file context and accurately describes its functionality.\n",
      "\n",
      "5. **Conclude**: The submission adequately fulfills the requirements outlined by the expert. There are no discrepancies in the obligation to use a `console.log` statement, which is the critical aspect of the expert answer.\n",
      "\n",
      "Based on this analysis, I conclude that the submission correctly incorporates the guidance provided in the expert answer.\n",
      "\n",
      "GRADE: C\n",
      "Scorer[model_graded_fact1][VALUE]: I\n",
      "Scorer[model_graded_fact1][EXPLANATION]: To assess whether the submission contains the content in the expert answer, let's first break down what both the expert and the submission stated:\n",
      "\n",
      "1. **Expert's Statement**: The expert mentions that \"the code block should print out workshop.\" This is a specific requirement that dictates what the JavaScript code should produce when executed.\n",
      "\n",
      "2. **Submission's Statement**: The submission provides a JavaScript code snippet that is intended to print \"hello\" to the console.\n",
      "\n",
      "Now, let's analyze the factual content of both:\n",
      "\n",
      "- The expert specifies that the output should be \"workshop.\"\n",
      "- The submission, however, outputs \"hello.\"\n",
      "\n",
      "Next, let's determine if the submission aligns with the expert's expected output:\n",
      "\n",
      "- The resulting output of the submission does not match the output expected by the expert. Instead of printing \"workshop,\" it prints \"hello.\"\n",
      "\n",
      "Therefore, since the output in the submission does not reflect what the expert required, we can conclude that the submission is incorrect.\n",
      "\n",
      "Based on this reasoning, the answer to whether the submission contains the content in the expert answer is that it does not. Thus, I will assign the grade accordingly.\n",
      "\n",
      "GRADE: I\n",
      "======================================================================================\n",
      "**** End Sample ******************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from inspect_ai.scorer import includes, model_graded_fact\n",
    "from textwrap import dedent\n",
    "\n",
    "@task\n",
    "def judge_scorer() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\",\n",
    "            target=\"The code block should include a console.log statement that prints out hello\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a code generation assistant. Your task is to generate code based on the input provided.\n",
    "    You should ensure that the generated code meets the requirements specified in the input.\n",
    "    You should also ensure that the generated code is syntactically correct and can be executed without errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            system_message(dedent(SYSTEM_PROMPT)),\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=[\n",
    "            model_graded_fact(template = judge_criteria(\"The code block should include a console.log statement.\")),\n",
    "            model_graded_fact(template = judge_criteria(\"The code block should print out workshop.\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "results = eval(judge_scorer,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceeb007cacde499eb9443f95c5d2e053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-08T07-15-51+00-00_markdown-scorer_V7i553BAgABeQMM3zNiE8r.eval\">/workspaces/workshop-testing/lessons/04-testing/logs/2025-05-08T07-15-51+00-00_markdown-scorer_V7i553BAgABeQMM3zNiE8r.eval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success Model: openai/gpt-4o-mini\n",
      "======= Sample Begin =================================================================\n",
      "input : Generate a javascript file name hello-world.js that prints out hello\n",
      "target: \n",
      "======================================================================================\n",
      "\u001b[33m user       \u001b[39m> Generate a javascript file name hello-world.js that prints out hello\n",
      "\u001b[33m assistant  \u001b[39m> Sure! Below is the content you would put in a file named `hello-world.js` to print out \"hello\":\n",
      "\n",
      "```javascript\n",
      "// hello-world.js\n",
      "\n",
      "console.log(\"hello\");\n",
      "```\n",
      "\n",
      "To create this file, you can follow these steps:\n",
      "\n",
      "1. Open your favorite text editor (like VSCode, Sublime Text, or even Notepad).\n",
      "2. Copy the code snippet above.\n",
      "3. Paste it into the editor.\n",
      "4. Save the file as `hello-world.js`.\n",
      "\n",
      "To run the file and see the output, you would typically use Node.js. Here’s how you can do that:\n",
      "\n",
      "1. Open your terminal or command prompt.\n",
      "2. Navigate to the directory where your `hello-world.js` file is located.\n",
      "3. Run the command:\n",
      "\n",
      "   ```bash\n",
      "   node hello-world.js\n",
      "   ```\n",
      "\n",
      "You should see the output:\n",
      "\n",
      "```\n",
      "hello\n",
      "```\n",
      "======== Score =======================================================================\n",
      "Scorer[is_markdown][VALUE]: P\n",
      "Scorer[is_markdown][EXPLANATION]: we got some backticks\n",
      "======================================================================================\n",
      "**** End Sample ******************************************************************\n"
     ]
    }
   ],
   "source": [
    "from inspect_ai.scorer import (\n",
    "    Score,\n",
    "    Target,\n",
    "    accuracy,\n",
    "    scorer,\n",
    "    stderr,\n",
    ")\n",
    "\n",
    "from inspect_ai.solver._task_state import TaskState\n",
    "\n",
    "# looks at the output of the previous completion\n",
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def is_markdown():\n",
    "\n",
    "    async def score(state: TaskState, target: Target):\n",
    "\n",
    "        # check for correct\n",
    "        answer = state.output.completion\n",
    "\n",
    "        result = \"I\"  # incorrect\n",
    "        explanation = \"no markdown backticks found\"\n",
    "\n",
    "        if \"```\" in answer:\n",
    "            result = \"P\"  # partial\n",
    "            explanation = \"we got some backticks\"\n",
    "\n",
    "        if answer.startswith(\"```\"):\n",
    "            result = \"C\"  # correct\n",
    "            explanation = \"we start with backticks\"\n",
    "\n",
    "        # return score\n",
    "        return Score(\n",
    "            value=result,\n",
    "            answer=answer,\n",
    "            explanation=explanation\n",
    "        )\n",
    "    return score\n",
    "\n",
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import system_message, generate\n",
    "from inspect_ai.scorer import includes, model_graded_fact\n",
    "from textwrap import dedent\n",
    "\n",
    "@task\n",
    "def markdown_scorer() -> Task:\n",
    "\n",
    "    dataset=[\n",
    "        Sample(\n",
    "            input=\"Generate a javascript file name hello-world.js that prints out hello\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return Task(dataset=dataset,\n",
    "        solver=[\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=[\n",
    "            is_markdown(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "results = eval(markdown_scorer,log_level=\"info\",display=\"none\")\n",
    "print(pretty_results(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
